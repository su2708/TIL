#TIL #스파르타코딩클럽 [[2411]]

## 1. Whisper
### Whisper
- OpenAI에서 개발한 다목적 음성-텍스트 변환 모델
- 웹에서 수집한 68만 시간의 다국어 데이터로 학습된 자동 음성 인식(ASR) 시스템
- 음성을 텍스트로 변환하거나 번역할 수 있는 능력을 갖춘 Transformer 기반의 딥러닝 모델


---
## 2. Whisper의 핵심 원리
### 핵심 원리
- Transformer 기반의 Encoder-Decoder 아키텍처를 사용
- 동작 순서
	1. 입력 음성: 음성 데이터를 시간-주파수 도메인으로 변환(Mel-Spectrogram)
	2. Tokenization: 입력 데이터를 토큰 단위로 변환
	3. Transformer Encoder: 음성 데이터를 인코딩하여 특징 추출
	4. Transformer Decoder: 텍스트로 디코딩
	5. 텍스트 출력: 최종 변환된 텍스트를 출력
![[whisper-model-architecture.svg]]


---
## 3. Whisper와 기술 발전
### 기존 음성 인식 기술의 한계
1. 언어와 억양의 다양성 부족
	- 특정 언어와 방언만을 지원하거나, 억양에 따라 달라지는 정확도가 문제
2. 환경 소음 민감성
	- 노이즈가 많은 환경에서의 성능이 낮음
3. 대규모 데이터 학습의 부족
	- 훈련 데이터의 크기와 다양성이 부족해 모델의 일반화 성능이 낮음


### Whisper와 기술 발전
1. 대규모 데이터 셋 학습
	- 다양한 언어, 억양, 소음 환경을 포함하는 수 십만 시간의 데이터로 훈련
	- 데이터 다양성 덕분에, 다국어 및 다양한 억양에서도 높은 정확도를 제공
2. End-to-End 처리
	- 기존의 음성 인식 모델은 음성->텍스트, 텍스트->번역 등의 과정으로 동작
	- Whisper는 하나의 모델 안에서 이를 처리하는 End-to-End 구조를 채택하여 처리 속도를 크게 향상 시키고, 중간 단계에서 발생하는 오류를 감소 시킴
3. Zero-shot 학습
	- 한 번도 학습되지 않은 언어, 억양, 또는 환경에서도 높은 정확도를 보여주는 Zero-shot Learning 능력을 보유


### Whisper의 미래
- 초 개인화 서비스: 음성 데이터를 활용한 개인 맞춤형 AI 비서
- 다문화 소통 증진: 언어 장벽을 제거하여 다문화 소통 강화
- 자동화 확대: 음성 데이터를 기반으로 한 효율적인 작업 자동화
