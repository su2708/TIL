#TIL #스파르타코딩클럽 [[2411]]

## 1. st.cache_data vs st.cache_resource
### st.cache_data
- 주로 데이터(ex. DataFrame, NumPy array, API query 등)를 캐싱하는데 사용
- 주기적으로 변경되거나 업데이트 될 수 있는 데이터(ex. 웹 API에서 가져오는 정보)를 캐싱할 때 유용
- 입력 인자와 반환값을 기반으로 데이터를 캐싱
- 데이터가 업데이트 되면 자동으로 캐시가 무효화되어 새로운 데이터를 가져옴
- 특정 입력이 주어졌을 때 동일한 반환값을 재사용 가능(decorator)
- 거의 대부분 `st.cache_data`를 사용


### st.cache_resource
- 주로 리소스 또는 외부 연결(ex. database 연결, API client 세션 등)을 캐싱하는데 사용
- 리소스 초기화에 비용이 많이 드는 작업(ex. database 연결)을 설정하는 경우, 매번 연결을 새로 하는 대신 동일한 연결을 재사용할 때 유용
- 변경 가능성이 낮고 초기화에 시간이 많이 소요되는 리소스에 적합
- 데이터 자체보다는 데이터와 연결된 리소스를 효율적으로 재사용

![[caching-high-level-diagram.png]]

---
## 2. 크롤링 (Crawling)
### 정적 크롤링 (Static Crawling)
- 서버가 처음부터 고정된 상태로 제공하는 정적인 콘텐츠를 가져오는 방식
- 주로 HTML 페이지에 미리 로드된 텍스트나 이미지 등이 크롤링 대상

- 특징
	- 서버에서 이미 완성된 HTML 파일을 받아오기 때문에 javascript 처리 없이 빠르게 크롤링 가능
	- 파일들을 단순히 다운로드 하는 방식이므로 구현이 비교적 쉬움
	- `requests`, `BeautifulSoup` 라이브러리 등으로 정적인 데이터를 쉽게 추출 가능

- 장점
	- 빠른 속도, 적은 자원 소모
	- javascript 렌더링이 필요하지 않아 구현이 간단
	- 추가적인 서버 요청이나 리소스 로딩 없이 HTML의 구조에만 의존해 간단하게 접근 가능

- 단점
	- javascript로 동적으로 생성된 콘텐츠는 가져올 수 없음
	- HTML 구조가 변경되면 데이터를 올바르게 가져오지 못함


### 동적 크롤링 (Dynamic Crawling)
- javascript로 동적으로 생성된 콘텐츠를 포함해 웹페이지의 모든 내용을 가져와 데이터를 추출하는 방식
- `Selenium`, `Playwright`와 같은 브라우저 자동화 도구를 통해 작업

- 특징
	- javascript를 실행하여 사용자에게 보여지는 최종 결과를 가져옴
	- 사용자 인터랙션을 자동화할 수 있어 모든 콘텐츠를 쉽게 추출 가능

- 장점
	- javascript로 로드되는 모든 컨텐츠를 가져오는 것이 가능
	- 사용자와 동일한 환경에서 웹페이지를 렌더링하기 때문에 크롤링의 완성도가 높음
	- 웹페이지와 상호작용이 필요한 콘텐츠에도 접근 가능

- 단점
	- 브라우저를 실행하기 때문에 속도가 느리고 자원 소모가 큼
	- 설정이 복잡하고 코드 작성이 정적 크롤링보다 난이도 높음
	- 계속적으로 서버 요청을 보내기 때문에 IP 차단 위험이 높아질 수 있음